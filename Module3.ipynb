{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Theory Questions\n"
      ],
      "metadata": {
        "id": "3XpoHlDSd6sF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.** KL is importan to the VAE loss function as without a proper ballence with reconstruction loss, the KL term can dominate leading to the model failing to learn anything meaningful from the latent space directly leading to low output quality. It can also cause gradients to shrink becoming inneffective for learning causing issues with parameter optimization."
      ],
      "metadata": {
        "id": "9v3PtYREeDMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.** The reparameterization trick allows backpropogation by turnign the random choice into a diffentiable function defined as such: z = ðœ‡ + ðœŽ.ðœ– where ðœ– is random noise. By doign this, the selection remains stochastic, while still ensuring gradients can flow back through the network to be optimized."
      ],
      "metadata": {
        "id": "hdkrqkw3fzUM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.** VAE's use a probabilistic latent space instead of a fixed one, which allows a single input to lead to different possible outputs. Rather than mapping an input to one exact point, the model learns a distribution in latent space and samples from it. This stochasticity is what lets VAEs generate diverse but still realistic data. If the latent space were fixed, each input would always produce the same output, so this variation wouldn't happen."
      ],
      "metadata": {
        "id": "uT2_mK9tg9Rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.** KL divergence helps make the latent space smooth by pushing the learned latent distributions to be close to a standard Gaussian distribution. This regularization prevents the encoder from placing points randomly or leaving gaps in latent space. Because of this, nearby points in the latent space decode to similar, realistic outputs, which makes it possible to sample from the space and still generate meaningful data."
      ],
      "metadata": {
        "id": "jgd9lkS6i9Z1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Coding Task"
      ],
      "metadata": {
        "id": "NEw5S7Kcd-fF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.0 VAE from example code"
      ],
      "metadata": {
        "id": "DEJacoyPltzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# VAE Architecture\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        # Encoder\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "        # Decoder\n",
        "        self.fc2 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.relu(self.fc1(x))\n",
        "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h2 = torch.relu(self.fc2(z))\n",
        "        return torch.sigmoid(self.fc3(h2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.view(-1, 3072)) # Updated input dim for encode\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "\n",
        "# Loss function\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(recon_x, x.view(-1, 3072), reduction='sum') # Changed to MSELoss and updated input dim\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + KLD # Updated to use recon_loss\n",
        "\n",
        "# Training the VAE\n",
        "def train(model, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch}, Loss: {train_loss / len(train_loader.dataset)}')"
      ],
      "metadata": {
        "id": "HQotnkbCltaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Task 1"
      ],
      "metadata": {
        "id": "_A2xNijUobVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, latent_dim=256):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder (Input: 3x32x32)\n",
        "        self.conv1 = nn.Conv2d(3, 16, 4, 2, 1)   # 3x32x32 -> 16x16x16\n",
        "        self.conv2 = nn.Conv2d(16, 32, 4, 2, 1)  # 16x16x16 -> 32x8x8\n",
        "        self.conv3 = nn.Conv2d(32, 128, 4, 2, 1) # 32x8x8 -> 128x4x4\n",
        "\n",
        "        # Flattened size: 128 channels * 4 * 4\n",
        "        self.fc_mu = nn.Linear(128*4*4, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(128*4*4, latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.fc_decode = nn.Linear(latent_dim, 128*4*4)\n",
        "\n",
        "        # Transposed convolutions to upsample back to 32x32\n",
        "        self.deconv1 = nn.ConvTranspose2d(128, 64, 4, 2, 1) # 128x4x4 -> 64x8x8\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, 4, 2, 1)  # 64x8x8 -> 32x16x16\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, 3, 4, 2, 1)   # 32x16x16 -> 3x32x32\n",
        "\n",
        "    def encode(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1) # Flatten\n",
        "        return self.fc_mu(x), self.fc_logvar(x)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        x = F.relu(self.fc_decode(z))\n",
        "        x = x.view(x.size(0), 128, 4, 4) # Reshape to feature map\n",
        "\n",
        "        x = F.relu(self.deconv1(x))\n",
        "        x = F.relu(self.deconv2(x))\n",
        "        x = torch.sigmoid(self.deconv3(x))  # Output [0,1]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n"
      ],
      "metadata": {
        "id": "7Lxd2FgG1iW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n"
      ],
      "metadata": {
        "id": "F-MkTYp5og3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions\n",
        "def loss_function_conv(recon_x, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kld\n"
      ],
      "metadata": {
        "id": "yfgM3myp-KFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Settup"
      ],
      "metadata": {
        "id": "8UHz0rasBe5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ConvVAE(latent_dim=128).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "num_epochs = 10\n",
        "\n",
        "# Training\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model(data)\n",
        "        loss = loss_function_conv(recon, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}, Avg Loss: {train_loss / len(train_loader.dataset):.4f}\")"
      ],
      "metadata": {
        "id": "8hJRGYxpgqu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.utils as vutils\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(16, 128).to(device)\n",
        "    samples = model.decode(z).cpu()\n",
        "    vutils.save_image(samples, \"generated_conv_vae.png\", nrow=4) # Adjusted nrow for saving"
      ],
      "metadata": {
        "id": "R0cV3hOZ_Oh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "grid = vutils.make_grid(samples.cpu(), nrow=4) # Adjusted nrow to 4 for a 4x4 grid\n",
        "plt.figure(figsize=(8,8)) # Adjusted figure size for better display of 4x4 grid\n",
        "plt.imshow(np.transpose(grid.numpy(), (1,2,0)))\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jnEHfmE6AAMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train and show images for Fully Connected VAE\n"
      ],
      "metadata": {
        "id": "D8fffNeiVksD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# VAE dimensions for CIFAR-10 (3x32x32 = 3072)\n",
        "input_dim = 3072\n",
        "hidden_dim = 400\n",
        "latent_dim = 20\n",
        "\n",
        "# Instantiate the VAE model\n",
        "model_VAE = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
        "\n",
        "# Initialize the Adam optimizer\n",
        "optimizer = optim.Adam(model_VAE.parameters(), lr=1e-3)\n",
        "\n",
        "# Define the number of epochs for training\n",
        "num_epochs = 10\n",
        "\n",
        "# Redefine loss_function here to ensure it uses the correct dimensions\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    recon_loss = F.mse_loss(recon_x, x.view(-1, input_dim), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + KLD\n",
        "\n",
        "print(f\"Training VAE on CIFAR-10 dataset for {num_epochs} epochs...\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model_VAE.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon_batch, mu, logvar = model_VAE(data)\n",
        "        loss = loss_function(recon_batch, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch}, Avg Loss: {train_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "#Generate and Display Samples\n",
        "\n",
        "model_VAE.eval() # Set the model to evaluation mode\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Generate random latent vectors\n",
        "    z = torch.randn(16, latent_dim).to(device) # Generate 16 samples\n",
        "\n",
        "    # Decode the latent vectors to generate images\n",
        "    generated_images = model_VAE.decode(z).cpu()\n",
        "\n",
        "    # Reshape the generated images back to 3D for CIFAR-10 (3 channels, 32x32)\n",
        "    generated_images = generated_images.view(-1, 3, 32, 32)\n",
        "\n",
        "    # Create a grid of images\n",
        "    grid = vutils.make_grid(generated_images, nrow=4, padding=2, normalize=True)\n",
        "\n",
        "    # Display the grid of images\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(grid.numpy(), (1, 2, 0)))\n",
        "    plt.title(\"Generated CIFAR-10 Samples (Fully Connected VAE)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QwVENS9zRaM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comparison"
      ],
      "metadata": {
        "id": "zYAr8eSmVjQn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although both the fully connected VAE and the convolutional VAE produced somewhat blurry images, the outputs from the convolutional model were noticeably easier to interpret. In several instances, identifiable features could be seen within the images. For example, the legs of a horse were clearly visible. In contrast, the images generated by the fully connected VAE lacked any distinguishable characteristics, making it difficult to recognize the subjects at all."
      ],
      "metadata": {
        "id": "Yeqp6xF3WcLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2 Interpolate"
      ],
      "metadata": {
        "id": "b4SNuhWu8nyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model into eval mode\n",
        "model.eval()\n",
        "\n",
        "data_iter = iter(train_loader)\n",
        "images, _ = next(data_iter)\n",
        "img1 = images[0].unsqueeze(0).to(device)\n",
        "img2 = images[1].unsqueeze(0).to(device)\n",
        "\n",
        "# Encode the images into latent space\n",
        "with torch.no_grad():\n",
        "    mu1, logvar1 = model.encode(img1)\n",
        "    mu2, logvar2 = model.encode(img2)\n",
        "\n",
        "\n",
        "    z1 = model.reparameterize(mu1, logvar1)\n",
        "    z2 = model.reparameterize(mu2, logvar2)\n",
        "\n",
        "# Interpolate between z1 and z2\n",
        "steps = 10 # Reduced number of interpolation steps\n",
        "z_list = []\n",
        "\n",
        "for alpha in torch.linspace(0, 1, steps):\n",
        "    z = z1 * (1 - alpha) + z2 * alpha\n",
        "    z_list.append(z)\n",
        "\n",
        "z_interp = torch.cat(z_list)\n",
        "\n",
        "# Decode the interpolated points\n",
        "with torch.no_grad():\n",
        "    interp_images = model.decode(z_interp).cpu()\n",
        "\n",
        "# Visualize Grid\n",
        "\n",
        "grid = vutils.make_grid(interp_images, nrow=steps) # Adjusted nrow to match steps\n",
        "plt.figure(figsize=(10,3)) # Adjusted figure size for fewer images\n",
        "plt.imshow(np.transpose(grid.numpy(), (1,2,0)))\n",
        "plt.axis('off')\n",
        "plt.title(\"Latent Space Interpolation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SQdaehTT8rg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 SVHN dataset"
      ],
      "metadata": {
        "id": "Hkq-6wMlDJPV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29f95e1d"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define transformations for SVHN (32x32 already)\n",
        "svhn_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                         (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Instantiate the SVHN dataset\n",
        "svhn_dataset = datasets.SVHN(\n",
        "    root='./data',\n",
        "    split='train',     \\\n",
        "    download=True,\n",
        "    transform=svhn_transform\n",
        ")\n",
        "\n",
        "# Create a DataLoader\n",
        "train_loader = DataLoader(\n",
        "    svhn_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"SVHN dataset and DataLoader created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Re-Train model"
      ],
      "metadata": {
        "id": "iFIG2bhBOULs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8f8078f"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Reinitialize the ConvVAE model\n",
        "model_SVHN = ConvVAE(latent_dim=128).to(device)\n",
        "\n",
        "# Reinitialize the Adam optimizer\n",
        "optimizer = optim.Adam(model_SVHN.parameters(), lr=1e-3)\n",
        "\n",
        "# Define the number of epochs for training\n",
        "num_epochs = 25\n",
        "\n",
        "print(f\"Training ConvVAE on SVHN dataset for {num_epochs} epochs...\")\n",
        "\n",
        "# Loop through the specified number of epochs, calling the train function\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    model_SVHN.train()\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        recon, mu, logvar = model_SVHN(data)\n",
        "        loss = loss_function_conv(recon, data, mu, logvar)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch}, Avg Loss: {total_loss / len(train_loader.dataset):.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Randomly sample from Latent Space\n"
      ],
      "metadata": {
        "id": "CdHu6uYMJorb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure model is in evaluation mode\n",
        "model_SVHN.eval()\n",
        "\n",
        "latent_dim = 128\n",
        "num_samples = 16\n",
        "\n",
        "# Random latent vectors\n",
        "z = torch.randn(num_samples, latent_dim).to(device)\n",
        "\n",
        "# Decode\n",
        "with torch.no_grad():\n",
        "    samples = model_SVHN.decode(z).cpu()\n",
        "\n",
        "# Plot 4x4 grid\n",
        "fig, axes = plt.subplots(4, 4, figsize=(6,6))\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    ax.imshow(samples[i].permute(1,2,0))  # CxHxW -> HxWxC\n",
        "    ax.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WxDdXqOxJt0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Interpolate bewteen two random vectors"
      ],
      "metadata": {
        "id": "zchANbahJ0Y3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick two random points\n",
        "z1 = torch.randn(1, latent_dim).to(device)\n",
        "z2 = torch.randn(1, latent_dim).to(device)\n",
        "\n",
        "num_interpolations = 10\n",
        "interpolated_imgs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for alpha in torch.linspace(0, 1, num_interpolations):\n",
        "        z = (1 - alpha) * z1 + alpha * z2\n",
        "        img = model_SVHN.decode(z)\n",
        "        interpolated_imgs.append(img.cpu())\n",
        "\n",
        "interpolated_imgs = torch.cat(interpolated_imgs, dim=0)\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, num_interpolations, figsize=(15,2))\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(interpolated_imgs[i].permute(1,2,0))\n",
        "    ax.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YIZRnqawJ0Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Varying latent dimensions"
      ],
      "metadata": {
        "id": "i1ewk8ewQQPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define SVHN test dataset and loader\n",
        "svhn_test_dataset = datasets.SVHN(\n",
        "    root='./data',\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=svhn_transform # Use the same transform as train\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    svhn_test_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "model_SVHN.eval()\n",
        "latent_dim = model_SVHN.fc_mu.out_features  # latent dimension size\n",
        "num_steps = 10\n",
        "values = torch.linspace(-5, 5, num_steps)\n",
        "\n",
        "#  Pick an image from the test set as base\n",
        "data_iter = iter(test_loader)\n",
        "img, _ = next(data_iter)\n",
        "img = img[0:1].to(device)  # pick first image in batch\n",
        "\n",
        "with torch.no_grad():\n",
        "    base_mu, _ = model_SVHN.encode(img)  # encode image to latent space\n",
        "\n",
        "# Detect \"active\" latent dimensions\n",
        "active_dims = []\n",
        "threshold = 1e-5  # variance threshold to consider a dimension active\n",
        "\n",
        "with torch.no_grad():\n",
        "    for dim in range(latent_dim):\n",
        "        imgs = []\n",
        "        for val in values:\n",
        "            z = base_mu.clone()\n",
        "            z[0, dim] = val\n",
        "            img_out = model_SVHN.decode(z).flatten()\n",
        "            imgs.append(img_out)\n",
        "        imgs = torch.stack(imgs)\n",
        "        var = imgs.var()\n",
        "        if var > threshold:\n",
        "            active_dims.append((dim, var))\n",
        "\n",
        "# Sort by variance and pick top 5 most active dimensions\n",
        "active_dims = sorted(active_dims, key=lambda x: x[1], reverse=True)[:5]\n",
        "active_dims = [d[0] for d in active_dims]  # just keep the dimension indices\n",
        "print(f\"Top 5 active latent dimensions: {active_dims}\")\n",
        "\n",
        "# Plot traversal for top 5 dimensions\n",
        "for dim in active_dims:\n",
        "    images = []\n",
        "    with torch.no_grad():\n",
        "        for val in values:\n",
        "            z = base_mu.clone()\n",
        "            z[0, dim] = val\n",
        "            img_out = model_SVHN.decode(z)\n",
        "            images.append(torch.clamp(img_out.cpu(), 0, 1))  # clamp to [0,1]\n",
        "\n",
        "    images = torch.cat(images, dim=0)\n",
        "\n",
        "    # Plot\n",
        "    fig, axes = plt.subplots(1, num_steps, figsize=(15, 2))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.imshow(images[i].permute(1,2,0))\n",
        "        ax.axis('off')\n",
        "    plt.suptitle(f\"Traversing latent dim {dim}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "a1WR_zppKb_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When traversing individual latent dimensions from a base latent vector, I observed that some dimensions had little to no effect on the reconstructed image, while others produced subtle changes in digit thickness, rotation, or color. Randomly sampling latent vectors from the prior produced plausible digit images, although some reconstructions were blurry or averaged, reflecting the smoothing effect of the VAE. Overall, the model's latent space captures continuous variations in digit appearance, but is not strong with respect to digit class."
      ],
      "metadata": {
        "id": "D4vSQXjpeFpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define SVHN test dataset and loader\n",
        "svhn_test_dataset = datasets.SVHN(\n",
        "    root='./data',\n",
        "    split='test',\n",
        "    download=True,\n",
        "    transform=svhn_transform # Use the same transform as train\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    svhn_test_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    drop_last=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "model_SVHN.eval()\n",
        "data_iter = iter(test_loader)\n",
        "images, _ = next(data_iter)\n",
        "images = images.to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    recon, _, _ = model_SVHN(images)\n",
        "\n",
        "# Show first 8 original and reconstructed images\n",
        "n = 8\n",
        "plt.figure(figsize=(16,4))\n",
        "for i in range(n):\n",
        "    # Original\n",
        "    ax = plt.subplot(2, n, i+1)\n",
        "    orig = (images[i] + 1)/2  # rescale original [-1,1] -> [0,1]\n",
        "    plt.imshow(orig.cpu().permute(1,2,0))\n",
        "    ax.axis('off')\n",
        "    if i == 0:\n",
        "        ax.set_title(\"Original\")\n",
        "\n",
        "    # Reconstruction\n",
        "    ax = plt.subplot(2, n, i+1+n)\n",
        "    recon_img = torch.clamp(recon[i], 0, 1)  # clamp to [0,1]\n",
        "    plt.imshow(recon_img.cpu().permute(1,2,0))\n",
        "    ax.axis('off')\n",
        "    if i == 0:\n",
        "        ax.set_title(\"Reconstruction\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "IKREFdSTLdPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reconstructed images generally capture the main shape and color of the original SVHN digits, but they often appear blurrier and less detailed. Fine features, such as sharp edges or exact background details, are smoothed out, and some reconstructions may appear as grey or black squares when the latent vector corresponds to a region the model has not learned well. This could potentially be fixed by extending the training over mroe epochs, but I did not expirement too much with this as the training time was taking quite awhile (I ran out of GPU usage with Colab) Overall, the reconstructions show that the ConvVAE captures the overall structure of the digits but loses some of the fine-grained detail present in the original images.\n",
        "\n"
      ],
      "metadata": {
        "id": "i5kcmv1qQHAJ"
      }
    }
  ]
}